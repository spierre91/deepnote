{"cells":[{"cell_type":"code","source":"%pip install openai\n%pip install catboost","metadata":{"tags":[],"cell_id":"0adf8006093a451895d31f046cb00961","allow_embed":"code_output","source_hash":"8c175969","execution_start":1673363490242,"execution_millis":11359,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Collecting openai\n  Downloading openai-0.26.0.tar.gz (54 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.7/54.7 KB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: requests>=2.20 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from openai) (2.28.1)\nRequirement already satisfied: aiohttp in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from openai) (3.8.3)\nRequirement already satisfied: tqdm in /shared-libs/python3.9/py/lib/python3.9/site-packages (from openai) (4.64.1)\nRequirement already satisfied: idna<4,>=2.5 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from requests>=2.20->openai) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from requests>=2.20->openai) (2022.9.24)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from requests>=2.20->openai) (1.26.12)\nRequirement already satisfied: charset-normalizer<3,>=2 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from requests>=2.20->openai) (2.1.1)\nRequirement already satisfied: frozenlist>=1.1.1 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from aiohttp->openai) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from aiohttp->openai) (4.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from aiohttp->openai) (1.2.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from aiohttp->openai) (1.8.1)\nRequirement already satisfied: attrs>=17.3.0 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from aiohttp->openai) (22.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from aiohttp->openai) (6.0.2)\nBuilding wheels for collected packages: openai\n  Building wheel for openai (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for openai: filename=openai-0.26.0-py3-none-any.whl size=66833 sha256=0521d4ca265f85c6ae8e6844f2a80bc750427b6b74cb99a5aba252e6d57d6259\n  Stored in directory: /root/.cache/pip/wheels/50/85/93/3c090d89fb182ca03a781eff1f7195ec4a893dbeea5ae964dc\nSuccessfully built openai\nInstalling collected packages: openai\nSuccessfully installed openai-0.26.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\nYou should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\nCollecting catboost\n  Downloading catboost-1.1.1-cp39-none-manylinux1_x86_64.whl (76.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.6/76.6 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: plotly in /shared-libs/python3.9/py/lib/python3.9/site-packages (from catboost) (5.10.0)\nRequirement already satisfied: scipy in /shared-libs/python3.9/py/lib/python3.9/site-packages (from catboost) (1.9.3)\nRequirement already satisfied: matplotlib in /shared-libs/python3.9/py/lib/python3.9/site-packages (from catboost) (3.6.0)\nCollecting graphviz\n  Downloading graphviz-0.20.1-py3-none-any.whl (47 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.0/47.0 KB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pandas>=0.24.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from catboost) (1.2.5)\nRequirement already satisfied: numpy>=1.16.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from catboost) (1.23.4)\nRequirement already satisfied: six in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from catboost) (1.16.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from pandas>=0.24.0->catboost) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from pandas>=0.24.0->catboost) (2022.5)\nRequirement already satisfied: cycler>=0.10 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from matplotlib->catboost) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from matplotlib->catboost) (4.37.4)\nRequirement already satisfied: pyparsing>=2.2.1 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from matplotlib->catboost) (3.0.9)\nRequirement already satisfied: kiwisolver>=1.0.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from matplotlib->catboost) (1.4.4)\nRequirement already satisfied: pillow>=6.2.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from matplotlib->catboost) (9.2.0)\nRequirement already satisfied: contourpy>=1.0.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from matplotlib->catboost) (1.0.5)\nRequirement already satisfied: packaging>=20.0 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from matplotlib->catboost) (21.3)\nRequirement already satisfied: tenacity>=6.2.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from plotly->catboost) (8.1.0)\nInstalling collected packages: graphviz, catboost\nSuccessfully installed catboost-1.1.1 graphviz-0.20.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\nYou should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import openai\nopenai.api_key = \"\"\n","metadata":{"tags":[],"cell_id":"f8dbdb5478f749439db51e9911865499","allow_embed":"code_output","source_hash":"df671972","execution_start":1673365907749,"execution_millis":1,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":50},{"cell_type":"code","source":"completion = openai.Completion.create(engine=\"text-davinci-003\", prompt=\"What is the pandas library?\", max_tokens=1000)\nprint(completion.choices[0]['text'])","metadata":{"tags":[],"cell_id":"a180f55a9c91496397774bef72d7973f","allow_embed":"code_output","source_hash":"76b98026","execution_start":1673365727131,"execution_millis":2539,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"\n\nPandas is an open source software library written in Python for data manipulation and analysis. Pandas is widely used in data science, machine learning and many other fields. It provides high-level data structures and tools for handling and manipulating data, including data frames, series, plotting tools and more.\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"completion = openai.Completion.create(engine=\"text-davinci-003\", prompt=\"what are some common Pandas use cases?\", max_tokens=240)\nprint(completion.choices[0]['text'])","metadata":{"tags":[],"cell_id":"2760765f78ac4f3b90287f6512348607","allow_embed":"code_output","source_hash":"e0759cfc","execution_start":1673363502724,"execution_millis":1964,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"\n\n1. Data Cleaning and Transformation\n2. Data Analysis and Exploration\n3. Time Series Analysis\n4. Data Visualization\n5. Statistical Modeling\n6. Predictive Modeling\n7. Machine Learning\n8. Web Scraping\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"#what are the most common deep learning libraries?\ncompletion = openai.Completion.create(engine=\"text-davinci-003\", prompt=\"what are the most common deep learning libraries?\", max_tokens=240)\nprint(completion.choices[0]['text'])","metadata":{"tags":[],"cell_id":"65cf1dd02cc7408bbcc2ee9fb0e64cb5","allow_embed":"code_output","source_hash":"1d073d9f","execution_start":1673363504691,"execution_millis":2997,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"\n\n1. TensorFlow \n2. PyTorch \n3. Keras \n4. Caffe \n5. CNTK \n6. MXNet \n7. Theano \n8. Deeplearning4j \n9. Gensim \n10. LUNA\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"completion = openai.Completion.create(engine=\"text-davinci-003\", prompt=\"What is a deep neural network?\", max_tokens=1000)\nprint(completion.choices[0]['text'])","metadata":{"tags":[],"cell_id":"13c87c1765004e44bcf533bad839146a","allow_embed":"code_output","source_hash":"fc1559af","execution_start":1673363507691,"execution_millis":2509,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"\n\nA deep neural network (DNN) is a type of artificial neural network (ANN) with multiple layers of neurons between the input and output layers. DNNs are designed to learn complex non-linear relationships from data, and have been successfully applied in a wide range of areas such as image recognition, natural language processing, and financial forecasting.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"completion = openai.Completion.create(engine=\"text-davinci-003\", prompt=\"give some ideas on feature transformations that can improve model performance\", max_tokens=1000)\nprint(completion.choices[0]['text'])","metadata":{"tags":[],"cell_id":"2413189d53874e22b3c645a0b1a09aee","allow_embed":"code_output","source_hash":"1cb10d06","execution_start":1673363510201,"execution_millis":5732,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"\n\n1. Standardization/Normalization: A common feature transform used to ensure features are on the same scale, standardizing or normalizing variables can help limit the severity of outliers and improve the overall model performance.\n\n2. Feature Binning: Binning is a process of transforming numerical variables into categorical ones. This can be useful when working with variables that have too many levels and can have a significant effect on the model performance.\n\n3. Polynomial Expansion: When a nonlinear relationship is expected between features and the output variable, a polynomial expansion feature transformation can help improve model performance.\n\n4. Feature Selection: Removing redundant or irrelevant features from the dataset can help improve the model performance as these features may lead to overfitting.\n\n5. Ensemble: Combining different types of models (or different versions of the same model) can often improve performance due to their combined capabilities.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"completion = openai.Completion.create(engine=\"text-davinci-003\", prompt=\"Write example python code that performs data standardization\", max_tokens=1000)\nprint(completion.choices[0]['text'])","metadata":{"tags":[],"cell_id":"af12900045b2478b93ab9736b64e882b","allow_embed":"code_output","source_hash":"b15dbb8b","execution_start":1673363515937,"execution_millis":4579,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"\n\n# Import the necessary libraries\nimport numpy as np\n\n# Define the data \ndata = np.array([[-3, 9, 0, 8],\n                 [ 4, 6, 5, 12],\n                 [20, 2, 3, 15]])\n\n# Calculate mean and standard deviation\nmean = np.mean(data, axis=0)\nstd = np.std(data, axis=0)\n\n# Perform data standardization\nstandardized_data = (data - mean) / std\n\n# Print the results\nprint(standardized_data)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Import the necessary libraries\nimport numpy as np\n\n# Define the data \ndata = np.array([[-3, 9, 0, 8],\n                 [ 4, 6, 5, 12],\n                 [20, 2, 3, 15]])\n\n# Calculate mean and standard deviation\nmean = np.mean(data, axis=0)\nstd = np.std(data, axis=0)\n\n# Perform data standardization\nstandardized_data = (data - mean) / std\n\n# Print the results\nprint(standardized_data)","metadata":{"tags":[],"cell_id":"32e7ec7fc04b4a29962cb69a6e1a25af","allow_embed":"code_output","source_hash":"2f0bcad0","execution_start":1673363633656,"execution_millis":4,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"[[-1.03881504  1.16247639 -1.29777137 -1.27872403]\n [-0.31164451  0.11624764  1.13554995  0.11624764]\n [ 1.35045955 -1.27872403  0.16222142  1.16247639]]\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"completion = openai.Completion.create(engine=\"text-davinci-003\", prompt=\"Write example python code that performs data normalization on fake data\", max_tokens=1000)\nprint(completion.choices[0]['text'])","metadata":{"tags":[],"cell_id":"a924a7e8372a4189b039844383357660","allow_embed":"code_output","source_hash":"a87e1c85","execution_start":1673363909417,"execution_millis":4670,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"\n\n# Normalizing data will rescale features in the range [0,1]\n\ndata = [3, 7, 10, 13] # Sample data\n\n# Calculate the maximum and minimum of the data\nmax_data = max(data)\nmin_data = min(data)\n\n# Normalize the data\nnormalized_data = [(x-min_data)/(max_data-min_data) for x in data]\n\n# Print first value to check \nprint(normalized_data[0]) # Prints 0.2\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"\n# Normalizing data will rescale features in the range [0,1]\n\ndata = [3, 7, 10, 13] # Sample data\n\n# Calculate the maximum and minimum of the data\nmax_data = max(data)\nmin_data = min(data)\n\n# Normalize the data\nnormalized_data = [(x-min_data)/(max_data-min_data) for x in data]\n\n# Print first value to check \nprint(normalized_data) \n","metadata":{"tags":[],"cell_id":"4dcd582d6443441b97b4652d53795756","allow_embed":"code_output","source_hash":"8d36fcfa","execution_start":1673363932199,"execution_millis":3,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"[0.0, 0.4, 0.7, 1.0]\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"completion = openai.Completion.create(engine=\"text-davinci-003\", prompt=\"How do i select a time series model?\", max_tokens=1000)\nprint(completion.choices[0]['text'])","metadata":{"tags":[],"cell_id":"3906c30630854d749bfee27b410c8b19","allow_embed":"code_output","source_hash":"431e6b6e","execution_start":1673363723013,"execution_millis":4807,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"\n\nThe selection of a time series model depends on the type of data that is being analyzed. If there are long-term trends or cycles in the data, then an autoregressive integrated moving average (ARIMA) model or a exponential smoothing (ETS) model may be the best option. If the data are seasonal or there are outliers present, then a Holt-Winters model may be a better choice. If the data do not contain any long-term trends or seasonality, a Box-Jenkins model may be suitable. Regardless of the model selected, it is important to consider the type of data that is being modeled when selecting a time series model.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"completion = openai.Completion.create(engine=\"text-davinci-003\", prompt=\"When to use ARIMA vs FB prophet?\", max_tokens=1000)\nprint(completion.choices[0]['text'])","metadata":{"tags":[],"cell_id":"dd459a0b0c2042f68abbe1c77c8cd789","allow_embed":"code_output","source_hash":"aa5a6656","execution_start":1673363744423,"execution_millis":3699,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"\n\nARIMA and FB Prophet are both used for time series forecasting. ARIMA is a linear model and is better for predicting long-term trends with stationary data. FB Prophet is a non-linear model and is better for predicting short-term trends with non-stationary data. So, you would use ARIMA when you are mainly concerned with predicting long-term trends, and you would use FB Prophet when you are mainly concerned with predicting short-term trends.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"completion = openai.Completion.create(engine=\"text-davinci-003\", prompt=\"Write example python code that generates synthetic healthcare readmission data stored in a dataframe\", max_tokens=1000)\nprint(completion.choices[0]['text'])","metadata":{"tags":[],"cell_id":"f997e7a71c57471c81dba210622b0ffe","allow_embed":"code_output","source_hash":"7160f974","execution_start":1673364364069,"execution_millis":5794,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"\n\nimport pandas as pd\nimport numpy as np\n\n# Create Dataframe \n\ndf = pd.DataFrame(columns=['Patient_ID', 'Age', 'Admission_Type', 'Readmitted'])\n\n# Generate Data\n\nnp.random.seed(0)\nfor i in range(10):\n  admission_type = np.random.choice(['Urgent', 'Scheduled', 'Emergency'])\n  patient_age = np.random.randint(18, 80)\n  readmission = np.random.choice([0, 1])\n  df.loc[i] = [i+1, patient_age, admission_type, readmission]\n\n# Print Dataframe to Console\n\nprint(df)\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Create Dataframe \n\ndf = pd.DataFrame(columns=['Patient_ID', 'Age', 'Admission_Type', 'Readmitted'])\n\n# Generate Data\n\nnp.random.seed(0)\nfor i in range(10):\n  admission_type = np.random.choice(['Urgent', 'Scheduled', 'Emergency'])\n  patient_age = np.random.randint(18, 80)\n  readmission = np.random.choice([0, 1])\n  df.loc[i] = [i+1, patient_age, admission_type, readmission]\n\n# Print Dataframe to Console\n\ndf\n","metadata":{"tags":[],"cell_id":"3b5d1a15336a44efbc67b9734d8ff466","allow_embed":"code_output","source_hash":"1f170973","execution_start":1673364389122,"execution_millis":22,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":23,"data":{"application/vnd.deepnote.dataframe.v3+json":{"column_count":4,"row_count":10,"columns":[{"name":"Patient_ID","dtype":"object","stats":{"unique_count":10,"nan_count":0,"categories":[{"name":"1","count":1},{"name":"2","count":1},{"name":"8 others","count":8}]}},{"name":"Age","dtype":"object","stats":{"unique_count":9,"nan_count":0,"categories":[{"name":"42","count":2},{"name":"65","count":1},{"name":"7 others","count":7}]}},{"name":"Admission_Type","dtype":"object","stats":{"unique_count":3,"nan_count":0,"categories":[{"name":"Urgent","count":4},{"name":"Emergency","count":4},{"name":"Scheduled","count":2}]}},{"name":"Readmitted","dtype":"object","stats":{"unique_count":2,"nan_count":0,"categories":[{"name":"1","count":8},{"name":"0","count":2}]}},{"name":"_deepnote_index_column","dtype":"int64"}],"rows":[{"Patient_ID":"1","Age":"65","Admission_Type":"Urgent","Readmitted":"1","_deepnote_index_column":"0"},{"Patient_ID":"2","Age":"21","Admission_Type":"Urgent","Readmitted":"1","_deepnote_index_column":"1"},{"Patient_ID":"3","Age":"37","Admission_Type":"Scheduled","Readmitted":"1","_deepnote_index_column":"2"},{"Patient_ID":"4","Age":"54","Admission_Type":"Emergency","Readmitted":"1","_deepnote_index_column":"3"},{"Patient_ID":"5","Age":"42","Admission_Type":"Emergency","Readmitted":"0","_deepnote_index_column":"4"},{"Patient_ID":"6","Age":"76","Admission_Type":"Urgent","Readmitted":"1","_deepnote_index_column":"5"},{"Patient_ID":"7","Age":"57","Admission_Type":"Emergency","Readmitted":"1","_deepnote_index_column":"6"},{"Patient_ID":"8","Age":"42","Admission_Type":"Emergency","Readmitted":"1","_deepnote_index_column":"7"},{"Patient_ID":"9","Age":"43","Admission_Type":"Scheduled","Readmitted":"1","_deepnote_index_column":"8"},{"Patient_ID":"10","Age":"27","Admission_Type":"Urgent","Readmitted":"0","_deepnote_index_column":"9"}]},"text/plain":"  Patient_ID Age Admission_Type Readmitted\n0          1  65         Urgent          1\n1          2  21         Urgent          1\n2          3  37      Scheduled          1\n3          4  54      Emergency          1\n4          5  42      Emergency          0\n5          6  76         Urgent          1\n6          7  57      Emergency          1\n7          8  42      Emergency          1\n8          9  43      Scheduled          1\n9         10  27         Urgent          0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Patient_ID</th>\n      <th>Age</th>\n      <th>Admission_Type</th>\n      <th>Readmitted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>65</td>\n      <td>Urgent</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>21</td>\n      <td>Urgent</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>37</td>\n      <td>Scheduled</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>54</td>\n      <td>Emergency</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>42</td>\n      <td>Emergency</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>76</td>\n      <td>Urgent</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>57</td>\n      <td>Emergency</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>42</td>\n      <td>Emergency</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>43</td>\n      <td>Scheduled</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>27</td>\n      <td>Urgent</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"completion = openai.Completion.create(engine=\"text-davinci-003\", prompt=\"Write example python code that generates synthetic healthcare readmission data stored in a dataframe. From this write code that builds a catboost model that predicts readmission outcomes. Also write code to calculate and print performance\", max_tokens=3000)\nprint(completion.choices[0]['text'])","metadata":{"tags":[],"cell_id":"7b8ff97243cb47a280d6a3295141fbe2","allow_embed":"code_output","source_hash":"ad1ce1e2","execution_start":1673364529638,"execution_millis":30591,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":" metrics\n\n## Generate Synthetic Healthcare Readmission Data\n\nimport pandas as pd \nimport numpy as np \n\n# set the seed for reproducibility \nnp.random.seed(1)\n\n# create dataframe \ndf = pd.DataFrame(np.random.randint(0, 100, size=(100, 10)), columns=['age','gender','length_of_stay','diagnosis','NIV','laboratory','past_hospitalizations','medications','bmi','readmission'])\n\n# add labels to data frame \ndf['age'] = np.random.randint(20, 80, size=(100))\ndf['gender'] = np.random.randint(1, 2, size=(100))\ndf['length_of_stay'] = np.random.randint(2, 14, size=(100))\ndf['diagnosis'] = np.random.randint(1, 5, size=(100))\ndf['NIV'] = np.random.randint(0, 2, size=(100))\ndf['laboratory'] = np.random.randint(1, 6, size=(100))\ndf['past_hospitalizations'] = np.random.randint(0, 10, size=(100))\ndf['medications'] = np.random.randint(1, 6, size=(100))\ndf['bmi'] = np.random.randint(18, 35, size=(100))\ndf['readmission'] = np.random.randint(0, 2, size=(100))\n\n# print the dataframe \nprint(df)\n\n## Build a CatBoost Model\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import confusion_matrix\n\n# separate X and y\nX = df.iloc[:, 0:9]\ny = df.iloc[:, 9]\n\n# initialize catboost classifier \ncat_clf = CatBoostClassifier(iterations=50,\n                            learning_rate=0.3,\n                            depth=8,\n                            eval_metric='Accuracy',\n                            random_seed=42)\n\n# fit the model \ncat_clf.fit(X, y)\n\n# predict values\ny_pred = cat_clf.predict(X)\n\n# print confusion matrix\nconf_mat = confusion_matrix(y, y_pred)\nprint(conf_mat)\n\n## Calculate and Print Performance Metrics\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\n\n# calculate performance metrics\nacc = accuracy_score(y, y_pred)\nprecision = precision_score(y, y_pred)\nrecall = recall_score(y, y_pred)\n\n# print performance metrics \nprint(\"Accuracy: {}\".format(acc))\nprint(\"Precision: {}\".format(precision))\nprint(\"Recall: {}\".format(recall))\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"## Generate Synthetic Healthcare Readmission Data\n\nimport pandas as pd \nimport numpy as np \n\n# set the seed for reproducibility \nnp.random.seed(1)\n\n# create dataframe \ndf = pd.DataFrame(np.random.randint(0, 100, size=(100, 10)), columns=['age','gender','length_of_stay','diagnosis','NIV','laboratory','past_hospitalizations','medications','bmi','readmission'])\n\n# add labels to data frame \ndf['age'] = np.random.randint(20, 80, size=(100))\ndf['gender'] = np.random.randint(1, 2, size=(100))\ndf['length_of_stay'] = np.random.randint(2, 14, size=(100))\ndf['diagnosis'] = np.random.randint(1, 5, size=(100))\ndf['NIV'] = np.random.randint(0, 2, size=(100))\ndf['laboratory'] = np.random.randint(1, 6, size=(100))\ndf['past_hospitalizations'] = np.random.randint(0, 10, size=(100))\ndf['medications'] = np.random.randint(1, 6, size=(100))\ndf['bmi'] = np.random.randint(18, 35, size=(100))\ndf['readmission'] = np.random.randint(0, 2, size=(100))\n\n# print the dataframe \nprint(df)\n\n## Build a CatBoost Model\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import confusion_matrix\n\n# separate X and y\nX = df.iloc[:, 0:9]\ny = df.iloc[:, 9]\n\n# initialize catboost classifier \ncat_clf = CatBoostClassifier(iterations=50,\n                            learning_rate=0.3,\n                            depth=8,\n                            eval_metric='Accuracy',\n                            random_seed=42)\n\n# fit the model \ncat_clf.fit(X, y)\n\n# predict values\ny_pred = cat_clf.predict(X)\n\n# print confusion matrix\nconf_mat = confusion_matrix(y, y_pred)\nprint(conf_mat)\n\n## Calculate and Print Performance Metrics\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\n\n# calculate performance metrics\nacc = accuracy_score(y, y_pred)\nprecision = precision_score(y, y_pred)\nrecall = recall_score(y, y_pred)\n\n# print performance metrics \nprint(\"Accuracy: {}\".format(acc))\nprint(\"Precision: {}\".format(precision))\nprint(\"Recall: {}\".format(recall))","metadata":{"tags":[],"cell_id":"bc718c65f54440b594932296ba2619e2","allow_embed":"code_output","source_hash":"2aa4be45","execution_start":1673364619253,"execution_millis":113,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"    age  gender  length_of_stay  diagnosis  NIV  laboratory  \\\n0    79       1              11          3    1           3   \n1    22       1              13          3    1           1   \n2    30       1               4          4    0           3   \n3    47       1               5          1    1           4   \n4    25       1              10          4    1           4   \n..  ...     ...             ...        ...  ...         ...   \n95   60       1               5          4    0           4   \n96   71       1               6          3    0           5   \n97   62       1              13          3    0           4   \n98   43       1               6          1    1           5   \n99   71       1               6          1    0           5   \n\n    past_hospitalizations  medications  bmi  readmission  \n0                       7            1   19            1  \n1                       6            1   27            0  \n2                       5            1   18            1  \n3                       7            2   30            1  \n4                       2            4   18            1  \n..                    ...          ...  ...          ...  \n95                      1            1   22            1  \n96                      4            1   32            0  \n97                      6            1   21            1  \n98                      8            3   26            1  \n99                      8            1   29            0  \n\n[100 rows x 10 columns]\n0:\tlearn: 0.6000000\ttotal: 171us\tremaining: 8.42ms\n1:\tlearn: 0.7800000\ttotal: 453us\tremaining: 10.9ms\n2:\tlearn: 0.8400000\ttotal: 681us\tremaining: 10.7ms\n3:\tlearn: 0.8300000\ttotal: 886us\tremaining: 10.2ms\n4:\tlearn: 0.8900000\ttotal: 1.1ms\tremaining: 9.88ms\n5:\tlearn: 0.9100000\ttotal: 1.31ms\tremaining: 9.63ms\n6:\tlearn: 0.9300000\ttotal: 1.52ms\tremaining: 9.32ms\n7:\tlearn: 0.9700000\ttotal: 1.71ms\tremaining: 8.99ms\n8:\tlearn: 0.9600000\ttotal: 1.92ms\tremaining: 8.75ms\n9:\tlearn: 0.9600000\ttotal: 2.01ms\tremaining: 8.04ms\n10:\tlearn: 0.9800000\ttotal: 2.22ms\tremaining: 7.86ms\n11:\tlearn: 0.9900000\ttotal: 2.42ms\tremaining: 7.67ms\n12:\tlearn: 0.9900000\ttotal: 2.61ms\tremaining: 7.44ms\n13:\tlearn: 0.9900000\ttotal: 2.81ms\tremaining: 7.21ms\n14:\tlearn: 0.9900000\ttotal: 3.02ms\tremaining: 7.04ms\n15:\tlearn: 1.0000000\ttotal: 3.22ms\tremaining: 6.84ms\n16:\tlearn: 1.0000000\ttotal: 3.42ms\tremaining: 6.64ms\n17:\tlearn: 1.0000000\ttotal: 3.64ms\tremaining: 6.47ms\n18:\tlearn: 1.0000000\ttotal: 3.88ms\tremaining: 6.32ms\n19:\tlearn: 1.0000000\ttotal: 4.08ms\tremaining: 6.12ms\n20:\tlearn: 1.0000000\ttotal: 4.27ms\tremaining: 5.89ms\n21:\tlearn: 1.0000000\ttotal: 4.46ms\tremaining: 5.68ms\n22:\tlearn: 1.0000000\ttotal: 4.67ms\tremaining: 5.48ms\n23:\tlearn: 1.0000000\ttotal: 4.9ms\tremaining: 5.3ms\n24:\tlearn: 1.0000000\ttotal: 5.11ms\tremaining: 5.11ms\n25:\tlearn: 1.0000000\ttotal: 5.3ms\tremaining: 4.89ms\n26:\tlearn: 1.0000000\ttotal: 5.5ms\tremaining: 4.69ms\n27:\tlearn: 1.0000000\ttotal: 5.73ms\tremaining: 4.5ms\n28:\tlearn: 1.0000000\ttotal: 5.94ms\tremaining: 4.3ms\n29:\tlearn: 1.0000000\ttotal: 6.14ms\tremaining: 4.09ms\n30:\tlearn: 1.0000000\ttotal: 6.34ms\tremaining: 3.88ms\n31:\tlearn: 1.0000000\ttotal: 6.54ms\tremaining: 3.68ms\n32:\tlearn: 1.0000000\ttotal: 6.75ms\tremaining: 3.48ms\n33:\tlearn: 1.0000000\ttotal: 6.95ms\tremaining: 3.27ms\n34:\tlearn: 1.0000000\ttotal: 7.17ms\tremaining: 3.07ms\n35:\tlearn: 1.0000000\ttotal: 7.37ms\tremaining: 2.87ms\n36:\tlearn: 1.0000000\ttotal: 7.59ms\tremaining: 2.67ms\n37:\tlearn: 1.0000000\ttotal: 7.8ms\tremaining: 2.46ms\n38:\tlearn: 1.0000000\ttotal: 8.01ms\tremaining: 2.26ms\n39:\tlearn: 1.0000000\ttotal: 8.2ms\tremaining: 2.05ms\n40:\tlearn: 1.0000000\ttotal: 8.43ms\tremaining: 1.85ms\n41:\tlearn: 1.0000000\ttotal: 8.64ms\tremaining: 1.65ms\n42:\tlearn: 1.0000000\ttotal: 8.84ms\tremaining: 1.44ms\n43:\tlearn: 1.0000000\ttotal: 9.04ms\tremaining: 1.23ms\n44:\tlearn: 1.0000000\ttotal: 9.24ms\tremaining: 1.03ms\n45:\tlearn: 1.0000000\ttotal: 9.46ms\tremaining: 822us\n46:\tlearn: 1.0000000\ttotal: 9.66ms\tremaining: 616us\n47:\tlearn: 1.0000000\ttotal: 9.87ms\tremaining: 411us\n48:\tlearn: 1.0000000\ttotal: 10.1ms\tremaining: 205us\n49:\tlearn: 1.0000000\ttotal: 10.3ms\tremaining: 0us\n[[53  0]\n [ 0 47]]\nAccuracy: 1.0\nPrecision: 1.0\nRecall: 1.0\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"completion = openai.Completion.create(engine=\"text-davinci-003\", prompt=\"Write example python code that generates synthetic transaction data stored in a dataframe\", max_tokens=1000)\nprint(completion.choices[0]['text'])","metadata":{"tags":[],"cell_id":"19a9d0f597574d908151c234cc4c44fe","allow_embed":"code_output","source_hash":"a82ce2a4","execution_start":1673364672681,"execution_millis":8025,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"\n\nimport pandas as pd \nimport numpy as np \n\n#create randomly generated customer data\ncustomer_id = np.arange(1,101) \ncustomer_names = [f'John Doe {x}' for x in range(1,101)] \n\n#create randomly generated transaction data\ntransaction_id = np.arange(1,101)\ndates = [f'2020-07-{x}' for x in range(1,101)]\namounts = np.random.randint(low=1, high=1000, size=(100,)) \n\n#create dataframe with randomly generated data\ntransaction_data = pd.DataFrame({'Customer ID': customer_id, \n                            'Customer Name': customer_names,\n                            'Transaction ID': transaction_id, \n                            'Date': dates, \n                            'Amount': amounts})\n\nprint(transaction_data)\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"\nimport pandas as pd \nimport numpy as np \n\n#create randomly generated customer data\ncustomer_id = np.arange(1,101) \ncustomer_names = [f'John Doe {x}' for x in range(1,101)] \n\n#create randomly generated transaction data\ntransaction_id = np.arange(1,101)\ndates = [f'2020-07-{x}' for x in range(1,101)]\namounts = np.random.randint(low=1, high=1000, size=(100,)) \n\n#create dataframe with randomly generated data\ntransaction_data = pd.DataFrame({'Customer ID': customer_id, \n                            'Customer Name': customer_names,\n                            'Transaction ID': transaction_id, \n                            'Date': dates, \n                            'Amount': amounts})\n\ntransaction_data","metadata":{"tags":[],"cell_id":"d786bdb042b744c2bda8eb6004a0ff77","allow_embed":"code_output","source_hash":"1307dbc7","execution_start":1673364707400,"execution_millis":52,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":31,"data":{"application/vnd.deepnote.dataframe.v3+json":{"column_count":5,"row_count":100,"columns":[{"name":"Customer ID","dtype":"int64","stats":{"unique_count":100,"nan_count":0,"min":"1","max":"100","histogram":[{"bin_start":1,"bin_end":10.9,"count":10},{"bin_start":10.9,"bin_end":20.8,"count":10},{"bin_start":20.8,"bin_end":30.700000000000003,"count":10},{"bin_start":30.700000000000003,"bin_end":40.6,"count":10},{"bin_start":40.6,"bin_end":50.5,"count":10},{"bin_start":50.5,"bin_end":60.400000000000006,"count":10},{"bin_start":60.400000000000006,"bin_end":70.3,"count":10},{"bin_start":70.3,"bin_end":80.2,"count":10},{"bin_start":80.2,"bin_end":90.10000000000001,"count":10},{"bin_start":90.10000000000001,"bin_end":100,"count":10}]}},{"name":"Customer Name","dtype":"object","stats":{"unique_count":100,"nan_count":0,"categories":[{"name":"John Doe 1","count":1},{"name":"John Doe 2","count":1},{"name":"98 others","count":98}]}},{"name":"Transaction ID","dtype":"int64","stats":{"unique_count":100,"nan_count":0,"min":"1","max":"100","histogram":[{"bin_start":1,"bin_end":10.9,"count":10},{"bin_start":10.9,"bin_end":20.8,"count":10},{"bin_start":20.8,"bin_end":30.700000000000003,"count":10},{"bin_start":30.700000000000003,"bin_end":40.6,"count":10},{"bin_start":40.6,"bin_end":50.5,"count":10},{"bin_start":50.5,"bin_end":60.400000000000006,"count":10},{"bin_start":60.400000000000006,"bin_end":70.3,"count":10},{"bin_start":70.3,"bin_end":80.2,"count":10},{"bin_start":80.2,"bin_end":90.10000000000001,"count":10},{"bin_start":90.10000000000001,"bin_end":100,"count":10}]}},{"name":"Date","dtype":"object","stats":{"unique_count":100,"nan_count":0,"categories":[{"name":"2020-07-1","count":1},{"name":"2020-07-2","count":1},{"name":"98 others","count":98}]}},{"name":"Amount","dtype":"int64","stats":{"unique_count":95,"nan_count":0,"min":"1","max":"999","histogram":[{"bin_start":1,"bin_end":100.8,"count":7},{"bin_start":100.8,"bin_end":200.6,"count":6},{"bin_start":200.6,"bin_end":300.4,"count":16},{"bin_start":300.4,"bin_end":400.2,"count":9},{"bin_start":400.2,"bin_end":500,"count":7},{"bin_start":500,"bin_end":599.8,"count":8},{"bin_start":599.8,"bin_end":699.6,"count":10},{"bin_start":699.6,"bin_end":799.4,"count":9},{"bin_start":799.4,"bin_end":899.1999999999999,"count":13},{"bin_start":899.1999999999999,"bin_end":999,"count":15}]}},{"name":"_deepnote_index_column","dtype":"int64"}],"rows":[{"Customer ID":"1","Customer Name":"John Doe 1","Transaction ID":"1","Date":"2020-07-1","Amount":"138","_deepnote_index_column":"0"},{"Customer ID":"2","Customer Name":"John Doe 2","Transaction ID":"2","Date":"2020-07-2","Amount":"373","_deepnote_index_column":"1"},{"Customer ID":"3","Customer Name":"John Doe 3","Transaction ID":"3","Date":"2020-07-3","Amount":"751","_deepnote_index_column":"2"},{"Customer ID":"4","Customer Name":"John Doe 4","Transaction ID":"4","Date":"2020-07-4","Amount":"385","_deepnote_index_column":"3"},{"Customer ID":"5","Customer Name":"John Doe 5","Transaction ID":"5","Date":"2020-07-5","Amount":"744","_deepnote_index_column":"4"},{"Customer ID":"6","Customer Name":"John Doe 6","Transaction ID":"6","Date":"2020-07-6","Amount":"56","_deepnote_index_column":"5"},{"Customer ID":"7","Customer Name":"John Doe 7","Transaction ID":"7","Date":"2020-07-7","Amount":"492","_deepnote_index_column":"6"},{"Customer ID":"8","Customer Name":"John Doe 8","Transaction ID":"8","Date":"2020-07-8","Amount":"622","_deepnote_index_column":"7"},{"Customer ID":"9","Customer Name":"John Doe 9","Transaction ID":"9","Date":"2020-07-9","Amount":"582","_deepnote_index_column":"8"},{"Customer ID":"10","Customer Name":"John Doe 10","Transaction ID":"10","Date":"2020-07-10","Amount":"267","_deepnote_index_column":"9"}]},"text/plain":"    Customer ID Customer Name  Transaction ID         Date  Amount\n0             1    John Doe 1               1    2020-07-1     138\n1             2    John Doe 2               2    2020-07-2     373\n2             3    John Doe 3               3    2020-07-3     751\n3             4    John Doe 4               4    2020-07-4     385\n4             5    John Doe 5               5    2020-07-5     744\n..          ...           ...             ...          ...     ...\n95           96   John Doe 96              96   2020-07-96     895\n96           97   John Doe 97              97   2020-07-97     984\n97           98   John Doe 98              98   2020-07-98     424\n98           99   John Doe 99              99   2020-07-99     294\n99          100  John Doe 100             100  2020-07-100     391\n\n[100 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Customer ID</th>\n      <th>Customer Name</th>\n      <th>Transaction ID</th>\n      <th>Date</th>\n      <th>Amount</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>John Doe 1</td>\n      <td>1</td>\n      <td>2020-07-1</td>\n      <td>138</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>John Doe 2</td>\n      <td>2</td>\n      <td>2020-07-2</td>\n      <td>373</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>John Doe 3</td>\n      <td>3</td>\n      <td>2020-07-3</td>\n      <td>751</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>John Doe 4</td>\n      <td>4</td>\n      <td>2020-07-4</td>\n      <td>385</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>John Doe 5</td>\n      <td>5</td>\n      <td>2020-07-5</td>\n      <td>744</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>96</td>\n      <td>John Doe 96</td>\n      <td>96</td>\n      <td>2020-07-96</td>\n      <td>895</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>97</td>\n      <td>John Doe 97</td>\n      <td>97</td>\n      <td>2020-07-97</td>\n      <td>984</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>98</td>\n      <td>John Doe 98</td>\n      <td>98</td>\n      <td>2020-07-98</td>\n      <td>424</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>99</td>\n      <td>John Doe 99</td>\n      <td>99</td>\n      <td>2020-07-99</td>\n      <td>294</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>100</td>\n      <td>John Doe 100</td>\n      <td>100</td>\n      <td>2020-07-100</td>\n      <td>391</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 5 columns</p>\n</div>"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"completion = openai.Completion.create(engine=\"text-davinci-003\", prompt=\"Write example python code that generates synthetic transaction data stored in a dataframe. Include customer ID, transaction amount, item ID, item name, age, gender, and zipcode\", max_tokens=2000)\nprint(completion.choices[0]['text'])","metadata":{"tags":[],"cell_id":"806cdf0729bf4fff9a912247c8352cfc","allow_embed":"code_output","source_hash":"ae8986e5","execution_start":1673364810801,"execution_millis":13798,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"\n\n\nimport pandas as pd\nimport numpy as np\n\nrows = ['customer_ID', 'transaction_amnt', 'item_ID', 'item_name', 'age', 'gender', 'zipcode']\n\ndata = pd.DataFrame(columns=rows)  \n\nfor i in range(1,100):\n        customer_ID = int( np.random.uniform(100,600-100)) \n        transaction_amnt = np.random.uniform(1.25, 10.00)\n        item_ID = int( np.random.uniform(1,35))\n        item_name = np.random.choice([\"phone\", \"tablet\", \"laptop\", \"smartwatch\"])\n        age = int( np.random.uniform(17,75)) \n        gender = np.random.choice([\"male\", \"female\"]) \n        zipcode = np.random.choice([\"98101\", \"98200\", \"98469\", \"98801\"])\n        data.loc[i] = [customer_ID, transaction_amnt, item_ID, item_name, age, gender, zipcode]\n\nprint (data)\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nrows = ['customer_ID', 'transaction_amnt', 'item_ID', 'item_name', 'age', 'gender', 'zipcode']\n\ndata = pd.DataFrame(columns=rows)  \n\nfor i in range(1,100):\n        customer_ID = int( np.random.uniform(100,600-100)) \n        transaction_amnt = np.random.uniform(1.25, 10.00)\n        item_ID = int( np.random.uniform(1,35))\n        item_name = np.random.choice([\"phone\", \"tablet\", \"laptop\", \"smartwatch\"])\n        age = int( np.random.uniform(17,75)) \n        gender = np.random.choice([\"male\", \"female\"]) \n        zipcode = np.random.choice([\"98101\", \"98200\", \"98469\", \"98801\"])\n        data.loc[i] = [customer_ID, transaction_amnt, item_ID, item_name, age, gender, zipcode]\n\ndata\n","metadata":{"tags":[],"cell_id":"8baa29620421412782c494da99099734","allow_embed":"code_output","source_hash":"e22e09cb","execution_start":1673364853257,"execution_millis":248,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":35,"data":{"application/vnd.deepnote.dataframe.v3+json":{"column_count":7,"row_count":99,"columns":[{"name":"customer_ID","dtype":"object","stats":{"unique_count":87,"nan_count":0,"categories":[{"name":"322","count":2},{"name":"146","count":2},{"name":"85 others","count":95}]}},{"name":"transaction_amnt","dtype":"float64","stats":{"unique_count":99,"nan_count":0,"min":"1.3676709942205518","max":"9.987054437035802","histogram":[{"bin_start":1.3676709942205518,"bin_end":2.229609338502077,"count":9},{"bin_start":2.229609338502077,"bin_end":3.091547682783602,"count":8},{"bin_start":3.091547682783602,"bin_end":3.953486027065127,"count":9},{"bin_start":3.953486027065127,"bin_end":4.815424371346652,"count":15},{"bin_start":4.815424371346652,"bin_end":5.677362715628177,"count":12},{"bin_start":5.677362715628177,"bin_end":6.539301059909702,"count":8},{"bin_start":6.539301059909702,"bin_end":7.401239404191227,"count":8},{"bin_start":7.401239404191227,"bin_end":8.263177748472753,"count":10},{"bin_start":8.263177748472753,"bin_end":9.125116092754277,"count":9},{"bin_start":9.125116092754277,"bin_end":9.987054437035802,"count":11}]}},{"name":"item_ID","dtype":"object","stats":{"unique_count":32,"nan_count":0,"categories":[{"name":"32","count":7},{"name":"14","count":6},{"name":"30 others","count":86}]}},{"name":"item_name","dtype":"object","stats":{"unique_count":4,"nan_count":0,"categories":[{"name":"tablet","count":28},{"name":"laptop","count":24},{"name":"2 others","count":47}]}},{"name":"age","dtype":"object","stats":{"unique_count":47,"nan_count":0,"categories":[{"name":"53","count":5},{"name":"26","count":4},{"name":"45 others","count":90}]}},{"name":"gender","dtype":"object","stats":{"unique_count":2,"nan_count":0,"categories":[{"name":"female","count":50},{"name":"male","count":49}]}},{"name":"zipcode","dtype":"object","stats":{"unique_count":4,"nan_count":0,"categories":[{"name":"98101","count":28},{"name":"98801","count":25},{"name":"2 others","count":46}]}},{"name":"_deepnote_index_column","dtype":"int64"}],"rows":[{"customer_ID":"321","transaction_amnt":"4.713675785008061","item_ID":"25","item_name":"laptop","age":"26","gender":"female","zipcode":"98101","_deepnote_index_column":"1"},{"customer_ID":"128","transaction_amnt":"9.813387045135537","item_ID":"9","item_name":"smartwatch","age":"43","gender":"male","zipcode":"98469","_deepnote_index_column":"2"},{"customer_ID":"490","transaction_amnt":"4.214857963236753","item_ID":"31","item_name":"phone","age":"48","gender":"female","zipcode":"98200","_deepnote_index_column":"3"},{"customer_ID":"322","transaction_amnt":"5.9467228169965605","item_ID":"14","item_name":"phone","age":"53","gender":"female","zipcode":"98801","_deepnote_index_column":"4"},{"customer_ID":"162","transaction_amnt":"1.6692301678150834","item_ID":"18","item_name":"laptop","age":"71","gender":"male","zipcode":"98200","_deepnote_index_column":"5"},{"customer_ID":"491","transaction_amnt":"5.1088079285776224","item_ID":"26","item_name":"tablet","age":"26","gender":"male","zipcode":"98101","_deepnote_index_column":"6"},{"customer_ID":"170","transaction_amnt":"9.780168421011425","item_ID":"10","item_name":"tablet","age":"61","gender":"male","zipcode":"98101","_deepnote_index_column":"7"},{"customer_ID":"319","transaction_amnt":"8.258702363235615","item_ID":"23","item_name":"tablet","age":"23","gender":"female","zipcode":"98101","_deepnote_index_column":"8"},{"customer_ID":"302","transaction_amnt":"3.994470838038586","item_ID":"32","item_name":"laptop","age":"65","gender":"female","zipcode":"98801","_deepnote_index_column":"9"},{"customer_ID":"489","transaction_amnt":"9.987054437035802","item_ID":"5","item_name":"laptop","age":"73","gender":"male","zipcode":"98801","_deepnote_index_column":"10"}]},"text/plain":"   customer_ID  transaction_amnt item_ID   item_name age  gender zipcode\n1          321          4.713676      25      laptop  26  female   98101\n2          128          9.813387       9  smartwatch  43    male   98469\n3          490          4.214858      31       phone  48  female   98200\n4          322          5.946723      14       phone  53  female   98801\n5          162          1.669230      18      laptop  71    male   98200\n..         ...               ...     ...         ...  ..     ...     ...\n95         195          9.636766      13       phone  47    male   98801\n96         425          8.315732      22  smartwatch  49  female   98101\n97         146          1.455586      19  smartwatch  69  female   98101\n98         438          8.426772      17       phone  26  female   98101\n99         246          4.782375       4      tablet  28    male   98101\n\n[99 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>customer_ID</th>\n      <th>transaction_amnt</th>\n      <th>item_ID</th>\n      <th>item_name</th>\n      <th>age</th>\n      <th>gender</th>\n      <th>zipcode</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>321</td>\n      <td>4.713676</td>\n      <td>25</td>\n      <td>laptop</td>\n      <td>26</td>\n      <td>female</td>\n      <td>98101</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>128</td>\n      <td>9.813387</td>\n      <td>9</td>\n      <td>smartwatch</td>\n      <td>43</td>\n      <td>male</td>\n      <td>98469</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>490</td>\n      <td>4.214858</td>\n      <td>31</td>\n      <td>phone</td>\n      <td>48</td>\n      <td>female</td>\n      <td>98200</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>322</td>\n      <td>5.946723</td>\n      <td>14</td>\n      <td>phone</td>\n      <td>53</td>\n      <td>female</td>\n      <td>98801</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>162</td>\n      <td>1.669230</td>\n      <td>18</td>\n      <td>laptop</td>\n      <td>71</td>\n      <td>male</td>\n      <td>98200</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>195</td>\n      <td>9.636766</td>\n      <td>13</td>\n      <td>phone</td>\n      <td>47</td>\n      <td>male</td>\n      <td>98801</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>425</td>\n      <td>8.315732</td>\n      <td>22</td>\n      <td>smartwatch</td>\n      <td>49</td>\n      <td>female</td>\n      <td>98101</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>146</td>\n      <td>1.455586</td>\n      <td>19</td>\n      <td>smartwatch</td>\n      <td>69</td>\n      <td>female</td>\n      <td>98101</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>438</td>\n      <td>8.426772</td>\n      <td>17</td>\n      <td>phone</td>\n      <td>26</td>\n      <td>female</td>\n      <td>98101</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>246</td>\n      <td>4.782375</td>\n      <td>4</td>\n      <td>tablet</td>\n      <td>28</td>\n      <td>male</td>\n      <td>98101</td>\n    </tr>\n  </tbody>\n</table>\n<p>99 rows × 7 columns</p>\n</div>"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"completion = openai.Completion.create(engine=\"text-davinci-003\", prompt=\" list some good public datasets\", max_tokens=1000)\nprint(completion.choices[0]['text'])","metadata":{"tags":[],"cell_id":"2aaf89e048084118b09d5dc8d5973997","allow_embed":"code_output","source_hash":"799d7981","execution_start":1673364891721,"execution_millis":2602,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"\n\n1. US Census Data\n2. Enron Email Dataset\n3. Global Open Data Index\n4. Air Quality Monitoring Data\n5. New York City Taxi Trip Data\n6. IMF Data\n7. World Bank Open Data\n8. Google Books Ngrams Dataset\n9. Amazon Reviews Dataset\n10. UCI Machine Learning Repository\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"completion = openai.Completion.create(engine=\"text-davinci-003\", prompt=\" list some good public datasets under apache 2.0 license. provide links to their source\", max_tokens=1000, temperature=0)\nprint(completion.choices[0]['text'])","metadata":{"tags":[],"cell_id":"dc005a6ff705440382a1d7733cad7653","allow_embed":"code_output","source_hash":"763a0cd0","execution_start":1673364936215,"execution_millis":7205,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"\n\n1. OpenStreetMap: https://www.openstreetmap.org/\n2. US Census Data: https://www.census.gov/data.html\n3. Google Books Ngrams: https://aws.amazon.com/datasets/google-books-ngrams/\n4. Wikipedia: https://dumps.wikimedia.org/enwiki/\n5. US Government Spending Data: https://www.usaspending.gov/\n6. World Bank Open Data: https://data.worldbank.org/\n7. Common Crawl: http://commoncrawl.org/\n8. Open Images: https://storage.googleapis.com/openimages/web/index.html\n9. OpenFlights: https://openflights.org/data.html\n10. GDELT: http://data.gdeltproject.org/\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"completion = openai.Completion.create(engine=\"text-davinci-003\", prompt=\" list some good public datasets under apache 2.0 license. provide links to their source and descriptions\", max_tokens=1000, temperature=0)\nprint(completion.choices[0]['text'])","metadata":{"tags":[],"cell_id":"8b63efe556cf4310ab44b844c761d2f5","allow_embed":"code_output","source_hash":"d9e6cfd6","execution_start":1673364954767,"execution_millis":11736,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"\n\n1. OpenStreetMap: OpenStreetMap is a free, editable map of the world, created and maintained by volunteers and available for use under an open license. It contains millions of data points, including roads, buildings, and points of interest. Source: https://www.openstreetmap.org/\n\n2. Google Books Ngrams: Google Books Ngrams is a dataset of over 5 million books from Google Books, spanning from 1500 to 2008. It contains word counts for each year, allowing researchers to track the usage of words over time. Source: https://aws.amazon.com/datasets/google-books-ngrams/\n\n3. Wikipedia: Wikipedia is a free, open-source encyclopedia with millions of articles in hundreds of languages. It is available for use under the Creative Commons Attribution-ShareAlike license. Source: https://www.wikipedia.org/\n\n4. Common Crawl: Common Crawl is a large-scale web crawl that collects data from over 5 billion webpages. It is available for use under the Apache 2.0 license. Source: https://commoncrawl.org/\n\n5. Open Images Dataset: The Open Images Dataset is a collection of 9 million images annotated with labels spanning over 6000 categories. It is available for use under the Apache 2.0 license. Source: https://storage.googleapis.com/openimages/web/index.html\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"completion = openai.Completion.create(engine=\"text-davinci-003\", prompt=\"What are some emerging machine learning use-cases in social media?\", max_tokens=1000, temperature=0)\nprint(completion.choices[0]['text'])","metadata":{"tags":[],"cell_id":"5eadc317599a441d886169d26d2060c1","allow_embed":"code_output","source_hash":"fa7443c9","execution_start":1673365110828,"execution_millis":4087,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"\n\n1. Automated Content Curation: Automatically curating content from social media platforms to create personalized content feeds for users.\n\n2. Sentiment Analysis: Analyzing user sentiment from social media posts to gain insights into customer opinions and preferences.\n\n3. Social Media Monitoring: Using machine learning algorithms to monitor social media conversations and detect potential issues or trends.\n\n4. Social Media Advertising: Leveraging machine learning to optimize social media advertising campaigns and target the right audience.\n\n5. Social Media Recommendations: Using machine learning to recommend content to users based on their interests and preferences.\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"completion = openai.Completion.create(engine=\"text-davinci-003\", prompt=\"What are some emerging machine learning use-cases in healthcare?\", max_tokens=1000, temperature=0)\nprint(completion.choices[0]['text'])","metadata":{"tags":[],"cell_id":"a08e5d3197504ba499f8de9b63c83090","allow_embed":"code_output","source_hash":"f3e455d","execution_start":1673365252249,"execution_millis":5894,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"\n\n1. Automated Diagnosis: Machine learning algorithms can be used to analyze patient data and medical images to detect and diagnose diseases.\n\n2. Personalized Medicine: Machine learning algorithms can be used to analyze patient data and medical images to create personalized treatment plans for each patient.\n\n3. Drug Discovery: Machine learning algorithms can be used to analyze large datasets of chemical compounds to identify potential new drugs.\n\n4. Clinical Decision Support: Machine learning algorithms can be used to analyze patient data and medical images to provide clinicians with real-time decision support.\n\n5. Predictive Analytics: Machine learning algorithms can be used to analyze patient data and medical images to predict future health outcomes.\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"completion = openai.Completion.create(engine=\"text-davinci-003\", prompt=\"What are some good research questions on using deep learning for image detection?\", max_tokens=1000, temperature=0)\nprint(completion.choices[0]['text'])","metadata":{"tags":[],"cell_id":"77d3e37fec824ba69d71991b9c73298e","allow_embed":"code_output","source_hash":"543c1ba7","execution_start":1673365541881,"execution_millis":7904,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"\n\n1. What are the most effective deep learning architectures for image detection?\n2. How can deep learning be used to improve the accuracy of image detection?\n3. What are the most effective methods for training deep learning models for image detection?\n4. How can deep learning be used to detect objects in images with varying levels of complexity?\n5. How can deep learning be used to detect objects in images with varying levels of illumination?\n6. How can deep learning be used to detect objects in images with varying levels of noise?\n7. How can deep learning be used to detect objects in images with varying levels of resolution?\n8. How can deep learning be used to detect objects in images with varying levels of occlusion?\n9. How can deep learning be used to detect objects in images with varying levels of background clutter?\n10. How can deep learning be used to detect objects in images with varying levels of rotation?\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"completion = openai.Completion.create(engine=\"text-davinci-003\", prompt=\"What are some good research questions related to NLP transformer models?\", max_tokens=1000, temperature=0)\nprint(completion.choices[0]['text'])","metadata":{"tags":[],"cell_id":"959960a41e784d279afadf9e891c0377","allow_embed":"code_output","source_hash":"649001ea","execution_start":1673365855245,"execution_millis":8775,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"\n\n1. How can transformer models be used to improve the accuracy of natural language processing tasks?\n2. What are the most effective methods for training transformer models for natural language processing tasks?\n3. How can transformer models be used to improve the efficiency of natural language processing tasks?\n4. What are the most effective methods for optimizing transformer models for natural language processing tasks?\n5. How can transformer models be used to improve the interpretability of natural language processing tasks?\n6. What are the most effective methods for deploying transformer models for natural language processing tasks?\n7. How can transformer models be used to improve the scalability of natural language processing tasks?\n8. What are the most effective methods for combining transformer models with other natural language processing techniques?\n9. How can transformer models be used to improve the robustness of natural language processing tasks?\n10. What are the most effective methods for evaluating transformer models for natural language processing tasks?\n","output_type":"stream"}],"execution_count":49},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=f655b25a-49ff-4873-9275-55f22845e8df' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"tags":[],"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"226ddc2337ad4c80a9c176feb573faf1","deepnote_persisted_session":{"createdAt":"2023-01-10T14:45:50.913Z"},"deepnote_execution_queue":[]}}